# ğŸŒ³ Decision Tree

A **Decision Tree** is a flowchart-like structure used for **classification** and **regression** tasks in machine learning. It splits data into branches based on feature values to make predictions or decisions.

---

## ğŸ“Œ Key Components

- **Root Node**: 
  The top node representing the best feature to split the dataset.

- **Internal Nodes**: 
  Decision points based on features (e.g., "Age < 30?").

- **Branches**: 
  Outcomes of the decisions (e.g., Yes/No).

- **Leaf Nodes**: 
  Final output or prediction (e.g., "Buy" or "Don't Buy").

---

## âœ… Example (Classification Tree)

Dataset to decide if a person buys a computer:

| Age   | Income | Student | Credit Rating | Buy Computer |
|-------|--------|---------|---------------|--------------|
| <30   | High   | No      | Fair          | No           |
| 30â€“40 | Medium | Yes     | Excellent     | Yes          |
| >40   | Low    | Yes     | Fair          | Yes          |

**Decision Tree:**

     [Age]
    /  |  \
 <30 30-40 >40
 /        \


---

## âš™ï¸ How It Works

1. **Choose the best feature to split** using:
   - Gini Impurity
   - Information Gain (Entropy)
   - Gain Ratio

2. **Recursively split** data based on feature values.

3. **Stop when**:
   - All samples in a node belong to the same class.
   - Maximum depth is reached.
   - No features left to split.

---

## ğŸ“ˆ Types of Decision Trees

- **Classification Tree**: Predicts class labels (e.g., spam or not).
- **Regression Tree**: Predicts continuous values (e.g., house price).

---

## ğŸ› ï¸ Implementation Libraries

### Python
- `sklearn.tree.DecisionTreeClassifier`
- `sklearn.tree.plot_tree()`

### R
- `rpart`, `C50`

### Java
- Weka (J48 = C4.5 algorithm)
- Apache Spark MLlib

---

## âš–ï¸ Pros and Cons

### âœ… Pros
- Simple to understand and interpret.
- Handles numerical and categorical data.
- Minimal preprocessing required.

### âŒ Cons
- Can easily **overfit**.
- Unstable to small data changes.
- Less accurate than ensemble methods like **Random Forests**.

---

## ğŸ§  Alternatives

- **Random Forest**: An ensemble of decision trees.
- **Gradient Boosted Trees**: Like XGBoost, LightGBM.
- **Logistic Regression**: For binary classification.
- **SVM**: For both linear and non-linear classification.

---

## ğŸ§ª Sample Code (Python - Scikit-learn)

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn import datasets
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Load dataset
iris = datasets.load_iris()
X, y = iris.data, iris.target

# Create and train model
clf = DecisionTreeClassifier()
clf.fit(X, y)

# Visualize the tree
plt.figure(figsize=(10,6))
plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)
plt.show()


# ğŸŒ¿ Splitting Factor in Decision Trees

In decision trees, a **splitting factor** (or **splitting criterion**) is the measure used to decide **which feature and threshold** to use when splitting the data at each node.

The goal is to choose the feature that best separates the data into classes (for classification) or minimizes error (for regression).

---

## ğŸ” Common Splitting Criteria

### 1. **Gini Impurity** (used in CART)
- Measures the **probability of incorrectly classifying** a randomly chosen element.
- Formula:


where `p_i` is the probability of class `i`.

- **Best split** is the one that results in the **lowest Gini impurity**.

---

### 2. **Entropy and Information Gain** (used in ID3)
- **Entropy** measures the disorder or impurity in the dataset
Entropy(D) = - âˆ‘ p_i * log2(p_i).



- **Information Gain (IG)** is the **reduction in entropy** after a dataset is split on a feature:

where `D_v` is the subset for value `v` of attribute `A`.

- **Best split** has the **highest information gain**.

---

### 3. **Gain Ratio** (used in C4.5)
- Adjusts **Information Gain** to **penalize attributes** with many distinct values.
- Formula:GainRatio = InformationGain / SplitInfo


- **Information Gain (IG)** is the **reduction in entropy** after a dataset is split on a feature:
IG(D, A) = Entropy(D) - âˆ‘ (|D_v| / |D|) * Entropy(D_v)

where `D_v` is the subset for value `v` of attribute `A`.

- **Best split** has the **highest information gain**.

---

### 3. **Gain Ratio** (used in C4.5)
- Adjusts **Information Gain** to **penalize attributes** with many distinct values.
- Formula: GainRatio = InformationGain / SplitInfo



- **SplitInfo** measures the potential information generated by splitting the dataset:
SplitInfo = - âˆ‘ (|D_v| / |D|) * log2(|D_v| / |D|)


---

### 4. **Mean Squared Error (MSE)** (for regression)
- Used when the target variable is **continuous**.
- Formula:MSE = (1/n) âˆ‘ (y_i - Å·_i)^2

- Choose the split that **minimizes MSE**.

---

## ğŸ“Š Comparison Table

| Criterion         | Task          | Chooses Split With...         |
|------------------|---------------|-------------------------------|
| Gini Impurity     | Classification | Lowest impurity               |
| Entropy / Info Gain | Classification | Highest information gain     |
| Gain Ratio         | Classification | Best balance of gain & value |
| MSE                | Regression     | Lowest squared error         |

---

## ğŸ§  Example

Given the following binary class split:

| Feature | Class |
|---------|-------|
| A       | 0     |
| A       | 0     |
| B       | 1     |
| B       | 1     |

- **Gini before split**: 0.5
- After splitting on `Feature`, each subset is pure â‡’ Gini = 0
- **Information Gain** = 0.5 (maximum)

Hence, this is a **perfect split**.

---

## ğŸ› ï¸ Implementation Note (Scikit-learn)

```python
from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(criterion="gini")  # or "entropy"
clf.fit(X, y)












